{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from twython import Twython\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import nltk.data\n",
    "tokens = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "sentiment=[]\n",
    "tweetId=[]\n",
    "\n",
    "\n",
    "\n",
    "emoticons_str =     r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [  # HTML tags\n",
    "               # @-mentions \n",
    "               # hash-tags\n",
    "               # URLs\n",
    "               # numbers\n",
    "               # words with - and '\n",
    "               # other words\n",
    "               # anything else\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',\n",
    "    r'(?:@[\\w_]+)',\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",\n",
    "    r'(?:[\\w_]+)',\n",
    "    r'(?:\\S)',\n",
    "    ]\n",
    "\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE\n",
    "                          | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('travelBan.csv')\n",
    "dataTravelBan=pd.DataFrame(data)\n",
    "\n",
    "data=pd.read_csv('parisAttacks.csv')\n",
    "dataParis=pd.DataFrame(data)\n",
    "\n",
    "data=pd.read_csv('syrianBombing.csv')\n",
    "dataSyria=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    \n",
    "    tokens = tokenize(s)\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    #tokens = [item.lower() for item in tokens]\n",
    "    tokens = [w for w in tokens if not w in stopset]\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "\n",
    "    prefixes = ('https','@', ':','rt')\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if not token.startswith(prefixes) and len(token) != 1:\n",
    "            if token not in new_tokens:\n",
    "                new_tokens.append(token)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    new_tokens = [lmtzr.lemmatize(token) for token in new_tokens]\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveDatabase(df):\n",
    "    tweetsPost={}\n",
    "    \n",
    "    for i,row in d.iterrows():\n",
    "        tweetTokenized=preprocess(row['Text'])\n",
    "        if tweetTokenized not in tweets:\n",
    "            tweets.append(tweetTokenized)\n",
    "            sentiment.append(row['Sentiments'])\n",
    "            location.append(row['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveDatabase(dataParis)\n",
    "saveDatabase(dataSyrian)\n",
    "saveDatabase(dataTravelBan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTrain=pd.DataFrame({'Tweets':tweets,'Sentiment':sentiment,'Location':location})\n",
    "X = f.Tweets\n",
    "y = f.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "train_transformed = []\n",
    "test_transformed = []\n",
    "list_y_train=y_train.tolist()\n",
    "list_y_test=y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in X_train:\n",
    "    train_transformed.append(' '.join(i))\n",
    "\n",
    "\n",
    "X_train_transformed=vect.fit_transform(train_transformed)\n",
    "\n",
    "\n",
    "\n",
    "for i in X_test:\n",
    "    test_transformed.append(' '.join(i))\n",
    "\n",
    "X_test_transformed = vect.transform(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_transformed, new_y_train)\n",
    "y_pred_class = nb.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(new_y_test, y_pred_class)\n",
    "metrics.accuracy_score(new_y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tokens = vect.get_feature_names()\n",
    "token_count = nb.feature_count_\n",
    "\n",
    "\n",
    "pos_token_count = nb.feature_count_[1, :]\n",
    "neg_token_count = nb.feature_count_[2, :]\n",
    "neu_token_count=nb.feature_count_[0, :]\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'neu':neutral_token_count, 'pos':pos_token_count,'neg':neg_token_count}).set_index('token')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
